{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 3D Data Management - Export Operations\n",
    "\n",
    "High-speed, concurrent export system for client delivery.\n",
    "Reads staged tasks and delivers files to client folders with absolute precision.\n",
    "\n",
    "**Mantra: Stability at hyperspeed**\n",
    "\n",
    "## Prerequisites\n",
    "- Google account with access to the 3D Data Management sheet\n",
    "- Staging folder access (read-only)\n",
    "- Client folder access (write-only)\n",
    "- Tasks with export_status='staged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Authentication & Setup - Thread-Safe Services\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "\n",
    "# Google API imports\n",
    "from google.colab import auth\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import pandas as pd\n",
    "import google.auth\n",
    "\n",
    "# Sheet ID - Update with your actual Sheet ID\n",
    "SHEET_ID = \"1HmDdq5g0Zk7d7Uodbh7fIUFq5XiONDZyvrBBZQgNK0w\"\n",
    "\n",
    "# Global thread-safe credentials\n",
    "_global_credentials = None\n",
    "thread_local = threading.local()\n",
    "\n",
    "def get_credentials():\n",
    "    \"\"\"Get shared credentials for all threads\"\"\"\n",
    "    global _global_credentials\n",
    "    if _global_credentials is None:\n",
    "        _global_credentials, _ = google.auth.default()\n",
    "    return _global_credentials\n",
    "\n",
    "def get_thread_safe_drive_service():\n",
    "    \"\"\"Thread-safe Drive service with automatic recreation\"\"\"\n",
    "    if not hasattr(thread_local, 'drive_service'):\n",
    "        credentials = get_credentials()\n",
    "        thread_local.drive_service = build('drive', 'v3', credentials=credentials)\n",
    "    return thread_local.drive_service\n",
    "\n",
    "def get_thread_safe_sheets_service():\n",
    "    \"\"\"Thread-safe Sheets service with automatic recreation\"\"\"\n",
    "    if not hasattr(thread_local, 'sheets_service'):\n",
    "        credentials = get_credentials()\n",
    "        thread_local.sheets_service = build('sheets', 'v4', credentials=credentials)\n",
    "    return thread_local.sheets_service\n",
    "\n",
    "def reset_thread_services():\n",
    "    \"\"\"Reset services on SSL errors\"\"\"\n",
    "    for attr in ['drive_service', 'sheets_service']:\n",
    "        if hasattr(thread_local, attr):\n",
    "            delattr(thread_local, attr)\n",
    "\n",
    "# Initialize authentication\n",
    "print(\"üîê Authenticating with Google...\")\n",
    "auth.authenticate_user()\n",
    "_global_credentials = get_credentials()\n",
    "\n",
    "# Test services\n",
    "try:\n",
    "    drive_service = get_thread_safe_drive_service()\n",
    "    sheets_service = get_thread_safe_sheets_service()\n",
    "    \n",
    "    # Quick auth test\n",
    "    about = drive_service.about().get(fields='user').execute()\n",
    "    user_email = about.get('user', {}).get('emailAddress', 'Unknown')\n",
    "    \n",
    "    print(f\"‚úÖ Authentication successful: {user_email}\")\n",
    "    print(f\"üìã Using Sheet ID: {SHEET_ID}\")\n",
    "    print(\"üöÄ Ready for export operations\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication failed: {e}\")\n",
    "    print(\"üí° Try: Runtime ‚Üí Restart Runtime, then re-run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration & Task Reader - Pure and Simple\n\nclass ConfigReader:\n    \"\"\"Reads configuration from sheet .env tab\"\"\"\n    \n    def __init__(self, sheet_id):\n        self.sheet_id = sheet_id\n        self.config = {}\n    \n    def load_config(self):\n        \"\"\"Load environment variables from .env sheet tab\"\"\"\n        sheets_service = get_thread_safe_sheets_service()\n        \n        result = sheets_service.spreadsheets().values().get(\n            spreadsheetId=self.sheet_id,\n            range='.env!A:B'\n        ).execute()\n        \n        for row in result.get('values', []):\n            if len(row) >= 2 and row[0] and row[1]:\n                self.config[row[0].strip()] = row[1].strip()\n        \n        # Validate required staging folder\n        if 'STAGING_FOLDER_ID' not in self.config:\n            raise ValueError(\"Missing STAGING_FOLDER_ID in .env configuration\")\n        \n        print(f\"‚öôÔ∏è Config loaded: {len(self.config)} variables\")\n        return self.config\n    \n    def get(self, key, default=None):\n        return self.config.get(key, default)\n\n\nclass StagedTaskReader:\n    \"\"\"Reads staged tasks ready for export\"\"\"\n    \n    def __init__(self, sheet_id):\n        self.sheet_id = sheet_id\n    \n    def get_export_batches(self):\n        \"\"\"Get available export batches with staged tasks\"\"\"\n        df = self._read_tasks_sheet()\n        \n        # Filter for staged tasks only\n        staged_tasks = df[df['Export Status'].str.lower() == 'staged']\n        \n        if staged_tasks.empty:\n            return []\n        \n        # Group by export batch\n        batches = staged_tasks.groupby('Export Batch').agg({\n            'Task ID': 'count',\n            'Export Time': 'first'\n        }).rename(columns={'Task ID': 'task_count'}).reset_index()\n        \n        batches = batches.sort_values('Export Time', ascending=False)\n        \n        return [{\n            'batch_id': row['Export Batch'],\n            'task_count': row['task_count'],\n            'created': row['Export Time']\n        } for _, row in batches.iterrows()]\n    \n    def get_staged_tasks(self, export_batch_id):\n        \"\"\"Get all staged tasks for a specific export batch\"\"\"\n        df = self._read_tasks_sheet()\n        \n        # Filter for specific batch and staged status\n        filter_mask = (\n            (df['Export Batch'] == export_batch_id) & \n            (df['Export Status'].str.lower() == 'staged')\n        )\n        \n        staged_tasks = df[filter_mask]\n        \n        return [{\n            'task_id': row['Task ID'],\n            'folder_name': row['Folder Name'],\n            'staging_folder_link': row['Production Folder'],  # Staging folder location\n            'batch_id': row['Batch ID'],\n            'export_batch_id': row['Export Batch']\n        } for _, row in staged_tasks.iterrows()]\n    \n    def _read_tasks_sheet(self):\n        \"\"\"Read and parse the main Tasks sheet\"\"\"\n        sheets_service = get_thread_safe_sheets_service()\n        \n        result = sheets_service.spreadsheets().values().get(\n            spreadsheetId=self.sheet_id,\n            range='Tasks!A:AC'\n        ).execute()\n        \n        values = result.get('values', [])\n        if not values:\n            return pd.DataFrame()\n        \n        headers = values[0]\n        data_rows = values[1:]\n        \n        # Pad rows to match header length\n        max_cols = len(headers)\n        padded_rows = [row + [''] * (max_cols - len(row)) for row in data_rows]\n        \n        return pd.DataFrame(padded_rows, columns=headers)\n\n\nprint(\"üìñ Configuration and Task Reader ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Export Operations - High-Speed File Delivery with Shared Drive Support\n\nclass ExportOperations:\n    \"\"\"Core export engine - moves files from staging to client with precision\"\"\"\n    \n    def __init__(self, staging_folder_id):\n        self.staging_folder_id = staging_folder_id\n        self.progress_lock = Lock()\n        self.completed_count = 0\n    \n    def validate_client_folder(self, client_folder_id):\n        \"\"\"Validate client folder access - FIXED for Shared Drives\"\"\"\n        try:\n            drive_service = get_thread_safe_drive_service()\n            \n            # Try regular Drive first, then Shared Drive if it fails\n            folder = None\n            try:\n                # Regular Drive API call\n                folder = drive_service.files().get(fileId=client_folder_id).execute()\n            except Exception as regular_error:\n                # If regular fails, try with Shared Drive support\n                try:\n                    folder = drive_service.files().get(\n                        fileId=client_folder_id,\n                        supportsAllDrives=True,\n                        supportsTeamDrives=True\n                    ).execute()\n                    print(\"‚úÖ Found folder in Shared Drive\")\n                except Exception as shared_error:\n                    return {\n                        'valid': False,\n                        'error': f'Cannot access folder: {str(regular_error)}'\n                    }\n            \n            # Verify it's actually a folder\n            if folder.get('mimeType') != 'application/vnd.google-apps.folder':\n                return {\n                    'valid': False,\n                    'error': 'Provided ID is not a folder'\n                }\n            \n            # Test if we can list contents (SAFE read-only check with Shared Drive support)\n            try:\n                drive_service.files().list(\n                    q=f\"'{client_folder_id}' in parents\",\n                    pageSize=1,\n                    fields='files(id,name)',\n                    supportsAllDrives=True,\n                    includeItemsFromAllDrives=True,\n                    supportsTeamDrives=True\n                ).execute()\n            except Exception as list_error:\n                return {\n                    'valid': False,\n                    'error': f'Cannot list folder contents: {str(list_error)}'\n                }\n            \n            return {\n                'valid': True,\n                'name': folder.get('name', 'Unknown'),\n                'id': client_folder_id,\n                'is_shared_drive': 'teamDriveId' in folder or 'driveId' in folder\n            }\n            \n        except Exception as e:\n            error_msg = str(e)\n            \n            # Parse common error messages\n            if 'not found' in error_msg.lower():\n                return {\n                    'valid': False,\n                    'error': 'Folder not found. For Shared Drives, ensure you have access and use the folder ID (not URL)'\n                }\n            elif 'permission' in error_msg.lower() or 'forbidden' in error_msg.lower():\n                return {\n                    'valid': False,\n                    'error': 'Insufficient permissions. For Shared Drives, you need at least \"Contributor\" access'\n                }\n            else:\n                return {\n                    'valid': False,\n                    'error': f'Access validation failed: {error_msg}'\n                }\n    \n    def export_tasks_concurrent(self, tasks, client_folder_id, max_workers=3):\n        \"\"\"Export tasks with safe concurrency\"\"\"\n        self.completed_count = 0\n        results = []\n        \n        chunk_size = 8  # Optimal chunk size for stability\n        actual_workers = min(max_workers, 3, len(tasks))\n        \n        print(f\"üöÄ Starting concurrent export: {len(tasks)} tasks\")\n        print(f\"‚ö° Mode: {actual_workers} workers, chunks of {chunk_size}\")\n        \n        total_start = time.time()\n        \n        # Process in chunks for stability\n        for chunk_start in range(0, len(tasks), chunk_size):\n            chunk_end = min(chunk_start + chunk_size, len(tasks))\n            chunk_tasks = tasks[chunk_start:chunk_end]\n            \n            print(f\"\\nüì¶ Chunk {chunk_start//chunk_size + 1}: tasks {chunk_start+1}-{chunk_end}\")\n            \n            with ThreadPoolExecutor(max_workers=actual_workers) as executor:\n                futures = {\n                    executor.submit(self._export_single_task, task, client_folder_id): task\n                    for task in chunk_tasks\n                }\n                \n                for future in as_completed(futures):\n                    result = future.result()\n                    results.append(result)\n            \n            # Brief pause between chunks\n            if chunk_end < len(tasks):\n                time.sleep(2)\n        \n        duration = time.time() - total_start\n        successful = sum(1 for r in results if r['success'])\n        \n        print(f\"\\nüéØ Export Complete: {successful}/{len(tasks)} in {duration:.1f}s\")\n        return results\n    \n    def _export_single_task(self, task, client_folder_id):\n        \"\"\"Export single task with automatic retry\"\"\"\n        max_retries = 3\n        base_delay = 1\n        \n        for attempt in range(max_retries):\n            try:\n                return self._perform_task_export(task, client_folder_id)\n                \n            except Exception as e:\n                if attempt < max_retries - 1:\n                    # Reset services on network errors\n                    if any(keyword in str(e).lower() for keyword in ['ssl', 'connection', 'timeout']):\n                        reset_thread_services()\n                    \n                    delay = base_delay * (2 ** attempt) + random.uniform(0, 1)\n                    time.sleep(delay)\n                    continue\n                else:\n                    return {\n                        'task_id': task['task_id'],\n                        'folder_name': task['folder_name'],\n                        'success': False,\n                        'error': str(e)\n                    }\n    \n    def _perform_task_export(self, task, client_folder_id):\n        \"\"\"Perform the actual file export for one task - FIXED for Shared Drives\"\"\"\n        start_time = time.time()\n        drive_service = get_thread_safe_drive_service()\n        \n        # Find staging folder for this task - ADD Shared Drive support\n        staging_folders = drive_service.files().list(\n            q=f\"'{self.staging_folder_id}' in parents and name contains '{task['export_batch_id']}' and mimeType='application/vnd.google-apps.folder'\",\n            fields='files(id,name)',\n            supportsAllDrives=True,\n            includeItemsFromAllDrives=True,\n            supportsTeamDrives=True\n        ).execute().get('files', [])\n        \n        if not staging_folders:\n            raise Exception(f\"Staging batch folder not found for {task['export_batch_id']}\")\n        \n        batch_folder_id = staging_folders[0]['id']\n        \n        # Find task folder within batch - ADD Shared Drive support\n        task_folders = drive_service.files().list(\n            q=f\"'{batch_folder_id}' in parents and name='{task['folder_name']}' and mimeType='application/vnd.google-apps.folder'\",\n            fields='files(id,name)',\n            supportsAllDrives=True,\n            includeItemsFromAllDrives=True,\n            supportsTeamDrives=True\n        ).execute().get('files', [])\n        \n        if not task_folders:\n            raise Exception(f\"Task folder '{task['folder_name']}' not found in staging\")\n        \n        staging_task_folder_id = task_folders[0]['id']\n        \n        # Find or create client task folder\n        client_task_folder_id = self._get_or_create_client_folder(\n            client_folder_id, task['folder_name']\n        )\n        \n        # Copy files with intelligent filtering\n        copied_count = self._copy_production_files(\n            staging_task_folder_id, \n            client_task_folder_id,\n            task['folder_name']\n        )\n        \n        duration = time.time() - start_time\n        \n        with self.progress_lock:\n            self.completed_count += 1\n            print(f\"‚úÖ ({self.completed_count}) {task['folder_name']}: {copied_count} files in {duration:.1f}s\")\n        \n        return {\n            'task_id': task['task_id'],\n            'folder_name': task['folder_name'],\n            'success': True,\n            'file_count': copied_count,\n            'duration': duration\n        }\n    \n    def _get_or_create_client_folder(self, client_folder_id, folder_name):\n        \"\"\"Get existing or create new task folder in client drive - FIXED for Shared Drives\"\"\"\n        drive_service = get_thread_safe_drive_service()\n        \n        # Check if folder already exists - ADD Shared Drive support\n        existing = drive_service.files().list(\n            q=f\"'{client_folder_id}' in parents and name='{folder_name}' and mimeType='application/vnd.google-apps.folder'\",\n            fields='files(id,name)',\n            supportsAllDrives=True,\n            includeItemsFromAllDrives=True,\n            supportsTeamDrives=True\n        ).execute().get('files', [])\n        \n        if existing:\n            return existing[0]['id']\n        \n        # Create new folder (ADD-ONLY operation) - ADD Shared Drive support\n        folder = drive_service.files().create(\n            body={\n                'name': folder_name,\n                'parents': [client_folder_id],\n                'mimeType': 'application/vnd.google-apps.folder'\n            },\n            supportsAllDrives=True,\n            supportsTeamDrives=True\n        ).execute()\n        \n        return folder['id']\n    \n    def _copy_production_files(self, source_folder_id, target_folder_id, folder_name):\n        \"\"\"Copy only production files, excluding original inputs - FIXED for Shared Drives\"\"\"\n        drive_service = get_thread_safe_drive_service()\n        \n        # Get all files in staging folder - ADD Shared Drive support\n        files = drive_service.files().list(\n            q=f\"'{source_folder_id}' in parents and trashed=false\",\n            fields='files(id,name,mimeType)',\n            supportsAllDrives=True,\n            includeItemsFromAllDrives=True,\n            supportsTeamDrives=True\n        ).execute().get('files', [])\n        \n        # Filter files - exclude original inputs and v0 files\n        exclude_files = ['image.jpg', 'img_mask.jpg', 'mask.jpg']\n        production_files = [\n            f for f in files \n            if f['mimeType'] != 'application/vnd.google-apps.folder'\n            and f['name'] not in exclude_files\n            and not re.search(r'_v0\\.', f['name'])  # Exclude version 0 files\n        ]\n        \n        copied_count = 0\n        \n        for file in production_files:\n            # Check if file already exists in client folder (SAFE check) - ADD Shared Drive support\n            existing = drive_service.files().list(\n                q=f\"'{target_folder_id}' in parents and name='{file['name']}'\",\n                fields='files(id)',\n                supportsAllDrives=True,\n                includeItemsFromAllDrives=True,\n                supportsTeamDrives=True\n            ).execute().get('files', [])\n            \n            if not existing:  # Only copy if doesn't exist (ADD-ONLY) - ADD Shared Drive support\n                drive_service.files().copy(\n                    fileId=file['id'],\n                    body={\n                        'name': file['name'],\n                        'parents': [target_folder_id]\n                    },\n                    supportsAllDrives=True,\n                    supportsTeamDrives=True\n                ).execute()\n                copied_count += 1\n        \n        return copied_count\n\n\nprint(\"üöÄ Export Operations engine ready (SAFE: ADD-ONLY operations + Shared Drive support)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Status Updater - Sheet Tracking\n\nclass ExportStatusUpdater:\n    \"\"\"Updates sheet with export delivery status\"\"\"\n    \n    def __init__(self, sheet_id):\n        self.sheet_id = sheet_id\n    \n    def update_delivery_status(self, export_results):\n        \"\"\"Update export status for all processed tasks\"\"\"\n        sheets_service = get_thread_safe_sheets_service()\n        \n        # Get current sheet data - Read full width to AC\n        result = sheets_service.spreadsheets().values().get(\n            spreadsheetId=self.sheet_id,\n            range='Tasks!A:AC'\n        ).execute()\n        \n        values = result.get('values', [])\n        if not values:\n            return\n        \n        headers = values[0]\n        \n        # Find column indices\n        task_id_col = self._find_column_index(headers, 'Task ID')\n        export_status_col = self._find_column_index(headers, 'Export Status')\n        \n        if task_id_col == -1 or export_status_col == -1:\n            print(\"‚ö†Ô∏è Required columns not found in sheet\")\n            return\n        \n        # Build batch updates\n        batch_updates = []\n        \n        for export_result in export_results:\n            task_id = export_result['task_id']\n            new_status = 'delivered' if export_result['success'] else 'delivery_failed'\n            \n            # Find task row\n            row_index = self._find_task_row(values, task_id, task_id_col)\n            if row_index > 0:\n                # Convert column index to Excel column letter\n                col_letter = self._index_to_column_letter(export_status_col)\n                \n                batch_updates.append({\n                    'range': f'Tasks!{col_letter}{row_index}',\n                    'values': [[new_status]]\n                })\n        \n        # Execute batch update\n        if batch_updates:\n            sheets_service.spreadsheets().values().batchUpdate(\n                spreadsheetId=self.sheet_id,\n                body={\n                    'valueInputOption': 'RAW',\n                    'data': batch_updates\n                }\n            ).execute()\n            \n            successful = sum(1 for r in export_results if r['success'])\n            print(f\"üìù Sheet updated: {successful} delivered, {len(export_results) - successful} failed\")\n    \n    def _find_column_index(self, headers, column_name):\n        \"\"\"Find column index by name\"\"\"\n        try:\n            return headers.index(column_name)\n        except ValueError:\n            return -1\n    \n    def _find_task_row(self, values, task_id, task_id_col):\n        \"\"\"Find row index for specific task ID\"\"\"\n        for i, row in enumerate(values[1:], start=2):  # Start at row 2 (1-indexed)\n            if len(row) > task_id_col and row[task_id_col] == task_id:\n                return i\n        return -1\n    \n    def _index_to_column_letter(self, index):\n        \"\"\"Convert column index to Excel letter (A, B, C, AA, AB, AC...)\"\"\"\n        letters = ''\n        while index >= 0:\n            letters = chr(65 + (index % 26)) + letters\n            index = index // 26 - 1\n        return letters\n\n\nprint(\"üìä Export Status Updater ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: CLI Interface - User-Guided Export Workflow\n\ndef run_export_workflow():\n    \"\"\"Interactive export workflow - guides user through entire process\"\"\"\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"üéØ 3D DATA MANAGEMENT - EXPORT OPERATIONS\")\n    print(\"=\"*60)\n    print(\"üìã Delivering staged files to client folders\")\n    \n    try:\n        # Step 1: Load configuration\n        print(\"\\n‚öôÔ∏è Loading configuration...\")\n        config_reader = ConfigReader(SHEET_ID)\n        config = config_reader.load_config()\n        \n        # Step 2: Get available export batches\n        print(\"\\nüìñ Reading staged tasks...\")\n        task_reader = StagedTaskReader(SHEET_ID)\n        batches = task_reader.get_export_batches()\n        \n        if not batches:\n            print(\"\\n‚ùå No staged export batches found\")\n            print(\"   Run staging operations first to prepare tasks for export\")\n            return\n        \n        # Step 3: Select export batch\n        print(f\"\\nüì¶ Available export batches:\")\n        for i, batch in enumerate(batches, 1):\n            print(f\"   {i}. {batch['batch_id']} ({batch['task_count']} tasks) - {batch['created']}\")\n        \n        while True:\n            try:\n                selection = input(f\"\\nüîç Select batch [1-{len(batches)}]: \").strip()\n                batch_index = int(selection) - 1\n                if 0 <= batch_index < len(batches):\n                    selected_batch = batches[batch_index]\n                    break\n                else:\n                    print(f\"Please enter a number between 1 and {len(batches)}\")\n            except ValueError:\n                print(\"Please enter a valid number\")\n        \n        print(f\"‚úÖ Selected: {selected_batch['batch_id']}\")\n        \n        # Step 4: Get tasks for selected batch\n        tasks = task_reader.get_staged_tasks(selected_batch['batch_id'])\n        print(f\"üìã Found {len(tasks)} staged tasks ready for export\")\n        \n        if len(tasks) <= 5:\n            for task in tasks:\n                print(f\"   ‚Ä¢ {task['folder_name']}\")\n        else:\n            for task in tasks[:3]:\n                print(f\"   ‚Ä¢ {task['folder_name']}\")\n            print(f\"   ... and {len(tasks) - 3} more\")\n        \n        # Step 5: Get client folder ID\n        print(\"\\nüéØ Client folder setup:\")\n        client_folder_id = input(\"   Enter client Google Drive folder ID: \").strip()\n        \n        if not client_folder_id:\n            print(\"‚ùå Client folder ID is required\")\n            return\n        \n        # Step 6: Validate client folder access\n        print(\"\\nüîç Validating client folder...\")\n        export_ops = ExportOperations(config['STAGING_FOLDER_ID'])\n        validation = export_ops.validate_client_folder(client_folder_id)\n        \n        if not validation['valid']:\n            print(f\"‚ùå Client folder validation failed: {validation['error']}\")\n            return\n        \n        print(f\"‚úÖ Client folder validated: {validation['name']}\")\n        \n        # Step 7: Final confirmation\n        print(f\"\\nüöÄ Ready to export:\")\n        print(f\"   üì¶ Batch: {selected_batch['batch_id']}\")\n        print(f\"   üìÅ Tasks: {len(tasks)}\")\n        print(f\"   üéØ Client: {validation['name']}\")\n        \n        confirm = input(f\"\\n‚úã Proceed with export? [y/N]: \").strip().lower()\n        if confirm != 'y':\n            print(\"‚ùå Export cancelled\")\n            return\n        \n        # Step 8: Execute export with progress tracking\n        print(f\"\\nüî• Starting export operation...\")\n        start_time = time.time()\n        \n        export_results = export_ops.export_tasks_concurrent(\n            tasks, \n            client_folder_id,\n            max_workers=3  # Conservative for stability\n        )\n        \n        # Step 9: Update sheet with results\n        print(\"\\nüìä Updating sheet status...\")\n        status_updater = ExportStatusUpdater(SHEET_ID)\n        status_updater.update_delivery_status(export_results)\n        \n        # Step 10: Final summary\n        duration = time.time() - start_time\n        successful = sum(1 for r in export_results if r['success'])\n        failed = len(export_results) - successful\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"üéâ EXPORT COMPLETE\")\n        print(\"=\"*60)\n        print(f\"üì¶ Batch: {selected_batch['batch_id']}\")\n        print(f\"üéØ Client: {validation['name']}\")\n        print(f\"‚úÖ Delivered: {successful}\")\n        print(f\"‚ùå Failed: {failed}\")\n        print(f\"‚è±Ô∏è Duration: {duration:.1f}s\")\n        \n        if failed > 0:\n            print(\"\\n‚ùå Failed tasks:\")\n            for result in export_results:\n                if not result['success']:\n                    print(f\"   ‚Ä¢ {result['folder_name']}: {result['error']}\")\n        \n        print(\"\\nüèÅ Export operation completed!\")\n        \n    except Exception as e:\n        print(f\"\\nüí• Export failed: {str(e)}\")\n        raise\n\n\n# Export workflow ready\nprint(\"\\nüéØ Export Workflow Ready!\")\nprint(\"üìû Run: run_export_workflow()\")\nprint(\"\\nüí° Make sure you have:\")\nprint(\"   ‚Ä¢ Staged tasks ready for export\")\nprint(\"   ‚Ä¢ Client Google Drive folder ID\")\nprint(\"   ‚Ä¢ Write access to client folder\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}