{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Data Management - Staging Operations\n",
    "\n",
    "High-speed, concurrent staging system for production tasks.\n",
    "Reads configuration from sheet, filters tasks, and copies folders to staging in parallel.\n",
    "\n",
    "## Prerequisites\n",
    "- Google account with access to the 3D Data Management sheet\n",
    "- Production and Staging folder access\n",
    "- Tasks with status='complete' and review_status='passed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup and Authentication with Shared Credentials\nimport os\nimport re\nimport time\nimport random\nimport threading\nimport socket\nimport ssl\nfrom datetime import datetime\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom threading import Lock\nimport uuid\n\n# Google API imports\nfrom google.colab import auth\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nimport pandas as pd\nfrom google.auth.transport.requests import Request\nimport google.auth\n\n# HARDCODED SHEET ID - Replace with your actual Sheet ID\nSHEET_ID = \"1HmDdq5g0Zk7d7Uodbh7fIUFq5XiONDZyvrBBZQgNK0w\"\n\n# Global credentials storage\n_global_credentials = None\nthread_local = threading.local()\n\ndef get_credentials():\n    \"\"\"Get shared credentials\"\"\"\n    global _global_credentials\n    if _global_credentials is None:\n        # Get default credentials from the authenticated session\n        _global_credentials, _ = google.auth.default()\n    return _global_credentials\n\ndef test_ssl_connection():\n    \"\"\"Test SSL connection to Google APIs\"\"\"\n    try:\n        context = ssl.create_default_context()\n        with socket.create_connection((\"www.googleapis.com\", 443), timeout=10) as sock:\n            with context.wrap_socket(sock, server_hostname=\"www.googleapis.com\") as ssock:\n                print(f\"‚úÖ SSL connection test: OK (Protocol: {ssock.version()})\")\n                return True\n    except Exception as e:\n        print(f\"‚ö†Ô∏è SSL connection test: Issues detected - {e}\")\n        return False\n\ndef test_auth_with_simple_call():\n    \"\"\"Test authentication with a simple API call\"\"\"\n    try:\n        # Try a simple Drive API call\n        drive_service = get_thread_safe_drive_service()\n        about = drive_service.about().get(fields='user').execute()\n        user_email = about.get('user', {}).get('emailAddress', 'Unknown')\n        print(f\"‚úÖ Authentication test: OK (User: {user_email})\")\n        return True\n    except Exception as e:\n        print(f\"‚ùå Authentication test: FAILED - {e}\")\n        return False\n\ndef get_thread_safe_drive_service():\n    \"\"\"Get thread-safe Drive service instance with shared credentials\"\"\"\n    if not hasattr(thread_local, 'drive_service'):\n        try:\n            credentials = get_credentials()\n            # Use credentials only (no custom http)\n            thread_local.drive_service = build('drive', 'v3', credentials=credentials)\n            print(f\"üîß Created new Drive service for thread {threading.current_thread().ident}\")\n        except Exception as e:\n            print(f\"‚ùå Failed to create Drive service: {e}\")\n            raise\n    return thread_local.drive_service\n\ndef get_thread_safe_sheets_service():\n    \"\"\"Get thread-safe Sheets service instance with shared credentials\"\"\"\n    if not hasattr(thread_local, 'sheets_service'):\n        try:\n            credentials = get_credentials()\n            # Use credentials only (no custom http)\n            thread_local.sheets_service = build('sheets', 'v4', credentials=credentials)\n            print(f\"üîß Created new Sheets service for thread {threading.current_thread().ident}\")\n        except Exception as e:\n            print(f\"‚ùå Failed to create Sheets service: {e}\")\n            raise\n    return thread_local.sheets_service\n\ndef reset_thread_services():\n    \"\"\"Reset thread-local services (useful after SSL errors)\"\"\"\n    if hasattr(thread_local, 'drive_service'):\n        delattr(thread_local, 'drive_service')\n    if hasattr(thread_local, 'sheets_service'):\n        delattr(thread_local, 'sheets_service')\n\ndef force_reauth():\n    \"\"\"Force re-authentication and refresh credentials\"\"\"\n    global _global_credentials\n    print(\"üîÑ Forcing re-authentication...\")\n    try:\n        # Clear cached services and credentials\n        reset_thread_services()\n        _global_credentials = None\n        \n        # Re-authenticate\n        auth.authenticate_user()\n        \n        # Get fresh credentials\n        _global_credentials = get_credentials()\n        \n        # Test with a simple call\n        if test_auth_with_simple_call():\n            print(\"‚úÖ Re-authentication successful\")\n            return True\n        else:\n            print(\"‚ùå Re-authentication failed\")\n            return False\n            \n    except Exception as e:\n        print(f\"‚ùå Re-authentication error: {e}\")\n        return False\n\nprint(\"\\nüîê Starting Enhanced OAuth authentication...\")\nauth.authenticate_user()\n\n# Initialize global credentials\ntry:\n    _global_credentials = get_credentials()\n    print(\"‚úÖ Credentials established\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Credentials setup issue: {e}\")\n\nprint(\"\\nüîç Running diagnostics...\")\ntest_ssl_connection()\nauth_ok = test_auth_with_simple_call()\n\nif not auth_ok:\n    print(\"\\n‚ö†Ô∏è Initial authentication failed. Attempting re-authentication...\")\n    auth_ok = force_reauth()\n\nif auth_ok:\n    # Initialize main thread services\n    drive_service = get_thread_safe_drive_service()\n    sheets_service = get_thread_safe_sheets_service()\n    \n    print(\"‚úÖ Authentication and service initialization complete.\")\n    print(f\"üìã Using Sheet ID: {SHEET_ID}\")\n    print(\"üõ°Ô∏è SSL error protection enabled with thread-safe service instances.\")\nelse:\n    print(\"‚ùå Authentication failed completely. Check your Google account permissions.\")\n    print(\"üí° Try: Runtime ‚Üí Factory Reset Runtime, then re-run this cell\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration Manager with Fixed Authentication\n\nclass ConfigManager:\n    def __init__(self, sheets_service, sheet_id):\n        self.sheets_service = sheets_service\n        self.sheet_id = sheet_id\n        self.config = {}\n        \n    def load_config(self):\n        \"\"\"Load configuration from .env tab in the sheet\"\"\"\n        try:\n            # Use thread-safe sheets service\n            sheets_service = get_thread_safe_sheets_service()\n            \n            # Read .env tab\n            range_name = '.env!A:B'\n            result = sheets_service.spreadsheets().values().get(\n                spreadsheetId=self.sheet_id, \n                range=range_name\n            ).execute()\n            \n            values = result.get('values', [])\n            \n            # Convert to config dictionary\n            for row in values:\n                if len(row) >= 2:\n                    key = row[0].strip()\n                    value = row[1].strip()\n                    if key and value:\n                        self.config[key] = value\n            \n            # Validate required keys\n            required_keys = ['PRODUCTION_FOLDER_ID', 'STAGING_FOLDER_ID']\n            missing_keys = [key for key in required_keys if key not in self.config]\n            \n            if missing_keys:\n                raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\n                \n            print(f\"‚úÖ Configuration loaded successfully\")\n            print(f\"üìÅ Production Folder: {self.config['PRODUCTION_FOLDER_ID']}\")\n            print(f\"üìÇ Staging Folder: {self.config['STAGING_FOLDER_ID']}\")\n            \n            return self.config\n            \n        except Exception as e:\n            error_msg = str(e)\n            if \"403\" in error_msg and \"unregistered callers\" in error_msg:\n                print(\"‚ùå Authentication Error: Google API credentials not properly established\")\n                print(\"üîß SOLUTION: Restart your Colab runtime and re-run authentication:\")\n                print(\"   1. Runtime ‚Üí Restart Runtime\")\n                print(\"   2. Re-run Cell 1 to authenticate\")\n                print(\"   3. Try again\")\n                print(f\"\\nüîç Also verify the sheet ID is correct: {self.sheet_id}\")\n            else:\n                print(f\"‚ùå Failed to load configuration: {str(e)}\")\n            raise\n    \n    def get(self, key, default=None):\n        \"\"\"Get configuration value\"\"\"\n        return self.config.get(key, default)\n\nprint(\"‚öôÔ∏è ConfigManager ready with authentication troubleshooting.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Task Reader with Thread-Safe Services\n\nclass TaskReader:\n    def __init__(self, sheets_service, sheet_id):\n        # Don't store the service - use thread-safe getter instead\n        self.sheet_id = sheet_id\n        \n    def get_stageable_tasks(self, batch_id_filter=None, agent_filter=None):\n        \"\"\"Get tasks that are ready for staging\"\"\"\n        try:\n            # Use thread-safe sheets service\n            sheets_service = get_thread_safe_sheets_service()\n            \n            # Read all task data - expanded to include columns beyond Z\n            range_name = 'Tasks!A:AZ'  # Extended range to include AB (Export Status)\n            result = sheets_service.spreadsheets().values().get(\n                spreadsheetId=self.sheet_id,\n                range=range_name\n            ).execute()\n            \n            values = result.get('values', [])\n            \n            if not values:\n                print(\"‚ùå No data found in Tasks sheet\")\n                return []\n            \n            # Get headers and create dataframe\n            headers = values[0]\n            data_rows = values[1:]\n            \n            # Find the maximum row length\n            max_cols = max(len(headers), max(len(row) for row in data_rows) if data_rows else 0)\n            \n            print(f\"üìã Headers: {len(headers)} columns, Max data: {max_cols} columns\")\n            \n            # Pad headers to match max columns\n            if len(headers) < max_cols:\n                for i in range(len(headers), max_cols):\n                    headers.append(f'Column_{chr(65 + i)}')  # Add Column_AA, Column_AB, etc.\n            \n            # Pad all data rows to match header length\n            padded_rows = []\n            for row in data_rows:\n                padded_row = row + [''] * (len(headers) - len(row))\n                padded_rows.append(padded_row)\n            \n            df = pd.DataFrame(padded_rows, columns=headers)\n            \n            # DEBUG: Show actual column values for filtering\n            print(\"\\nüîç DEBUGGING FILTER COLUMNS:\")\n            \n            # Check Status column\n            if 'Status' in df.columns:\n                status_counts = df['Status'].value_counts()\n                print(f\"üìä Status column values: {dict(status_counts)}\")\n            \n            # Check Review Status column  \n            if 'Review Status' in df.columns:\n                review_counts = df['Review Status'].value_counts()\n                print(f\"üìä Review Status column values: {dict(review_counts)}\")\n            \n            # Check Export-related columns\n            export_cols = ['Export Status', 'Export Batch', 'Export Time']\n            for col in export_cols:\n                if col in df.columns:\n                    # Count non-empty values\n                    non_empty = df[col].fillna('').astype(str).str.strip()\n                    non_empty_count = sum(non_empty != '')\n                    empty_count = len(df) - non_empty_count\n                    print(f\"üìä {col}: {non_empty_count} non-empty, {empty_count} empty\")\n                    if non_empty_count > 0:\n                        # Show sample non-empty values\n                        sample_values = non_empty[non_empty != ''].unique()[:5]\n                        print(f\"    Sample values: {list(sample_values)}\")\n                else:\n                    print(f\"‚ùå '{col}' column not found\")\n            \n            print(\"üîç END DEBUG\\n\")\n            \n            # Filter for stageable tasks with proper null handling\n            # Clean and normalize the status values\n            df['Status_clean'] = df['Status'].fillna('').astype(str).str.strip().str.lower()\n            df['Review_Status_clean'] = df['Review Status'].fillna('').astype(str).str.strip().str.lower()\n            \n            # Check if task has been processed for export (has ANY value in export columns)\n            export_processed = pd.Series([False] * len(df))\n            \n            for col in export_cols:\n                if col in df.columns:\n                    col_clean = df[col].fillna('').astype(str).str.strip()\n                    export_processed = export_processed | (col_clean != '')\n            \n            # Apply filters: status=complete AND review_status=passed AND NOT already processed for export\n            stageable_filter = (\n                (df['Status_clean'] == 'complete') & \n                (df['Review_Status_clean'] == 'passed') &\n                (~export_processed)  # NOT already processed for export\n            )\n            \n            print(f\"üéØ Tasks with status='complete': {sum(df['Status_clean'] == 'complete')}\")\n            print(f\"üéØ Tasks with review_status='passed': {sum(df['Review_Status_clean'] == 'passed')}\")\n            print(f\"üéØ Tasks already processed for export: {sum(export_processed)}\")\n            print(f\"üéØ Tasks NOT processed for export: {sum(~export_processed)}\")\n            print(f\"üéØ Tasks meeting ALL criteria: {sum(stageable_filter)}\")\n            \n            stageable = df[stageable_filter].copy()\n            \n            # Apply additional filters\n            if batch_id_filter:\n                before_batch_filter = len(stageable)\n                stageable = stageable[stageable['Batch ID'] == batch_id_filter]\n                print(f\"üîç After batch filter '{batch_id_filter}': {len(stageable)} (was {before_batch_filter})\")\n            \n            if agent_filter:\n                before_agent_filter = len(stageable)\n                stageable = stageable[\n                    stageable['Agent Email'].str.contains(agent_filter, case=False, na=False)\n                ]\n                print(f\"üîç After agent filter '{agent_filter}': {len(stageable)} (was {before_agent_filter})\")\n            \n            # Convert to list of task objects\n            tasks = []\n            for _, row in stageable.iterrows():\n                task = {\n                    'task_id': row['Task ID'],\n                    'folder_name': row['Folder Name'],\n                    'production_folder_id': self._extract_folder_id(row['Production Folder']),\n                    'batch_id': row['Batch ID'],\n                    'agent_email': row['Agent Email'],\n                    'group': row.get('Group', ''),\n                    'review_score': row.get('Review Score', ''),\n                    'status': row['Status'],\n                    'review_status': row['Review Status'],\n                    'export_status': row.get('Export Status', ''),\n                    'export_batch': row.get('Export Batch', ''),\n                    'export_time': row.get('Export Time', '')\n                }\n                tasks.append(task)\n            \n            print(f\"\\nüìä Found {len(tasks)} tasks ready for staging\")\n            if len(tasks) > 0:\n                sample = tasks[0]\n                print(f\"üìã Sample task: {sample['folder_name']}\")\n                print(f\"    Status: '{sample['status']}', Review: '{sample['review_status']}'\")\n                print(f\"    Export Status: '{sample['export_status']}', Export Batch: '{sample['export_batch']}'\")\n                \n            return tasks\n            \n        except Exception as e:\n            print(f\"‚ùå Failed to read tasks: {str(e)}\")\n            raise\n    \n    def _extract_folder_id(self, folder_link):\n        \"\"\"Extract Google Drive folder ID from URL or return as-is if already an ID\"\"\"\n        if not folder_link:\n            return None\n            \n        # Try to extract from URL\n        match = re.search(r'folders/([a-zA-Z0-9-_]+)', folder_link)\n        if match:\n            return match.group(1)\n        \n        # Try to match direct ID pattern\n        if re.match(r'^[a-zA-Z0-9-_]{20,}$', folder_link):\n            return folder_link\n        \n        return folder_link  # Return as-is and let Drive API handle it\n\nprint(\"üìñ TaskReader ready with thread-safe services.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Thread-Safe Drive Operations\n\nclass DriveOperations:\n    def __init__(self, drive_service=None):\n        # Don't store the service - use thread-safe getter instead\n        self.progress_lock = Lock()\n        self.completed_count = 0\n        \n    def create_batch_folder(self, staging_folder_id, batch_id):\n        \"\"\"Create batch folder in staging\"\"\"\n        try:\n            drive_service = get_thread_safe_drive_service()\n            timestamp = datetime.now().strftime('%Y-%m-%d')\n            folder_name = f\"Export_{timestamp}_{batch_id}\"\n            \n            folder_metadata = {\n                'name': folder_name,\n                'parents': [staging_folder_id],\n                'mimeType': 'application/vnd.google-apps.folder'\n            }\n            \n            folder = drive_service.files().create(\n                body=folder_metadata,\n                fields='id,name,webViewLink'\n            ).execute()\n            \n            print(f\"üìÅ Created batch folder: {folder_name}\")\n            return {\n                'id': folder.get('id'),\n                'name': folder.get('name'),\n                'url': folder.get('webViewLink')\n            }\n            \n        except Exception as e:\n            print(f\"‚ùå Failed to create batch folder: {str(e)}\")\n            raise\n    \n    def copy_task_folder(self, task, batch_folder_id):\n        \"\"\"Copy single task folder to staging with thread-safe retry logic\"\"\"\n        start_time = time.time()\n        max_retries = 5\n        base_delay = 1\n        \n        for attempt in range(max_retries):\n            try:\n                # Get thread-safe Drive service\n                drive_service = get_thread_safe_drive_service()\n                \n                # Create task subfolder\n                task_folder_metadata = {\n                    'name': task['folder_name'],\n                    'parents': [batch_folder_id],\n                    'mimeType': 'application/vnd.google-apps.folder'\n                }\n                \n                task_folder = drive_service.files().create(\n                    body=task_folder_metadata,\n                    fields='id,name'\n                ).execute()\n                \n                task_folder_id = task_folder.get('id')\n                \n                # List files in production folder with retry\n                production_folder_id = task['production_folder_id']\n                files_query = f\"'{production_folder_id}' in parents and trashed=false\"\n                \n                files_result = self._thread_safe_api_call(\n                    lambda svc: svc.files().list(\n                        q=files_query,\n                        fields='files(id,name,mimeType)'\n                    ).execute()\n                )\n                \n                files = files_result.get('files', [])\n                copied_count = 0\n                \n                # Copy each file with enhanced retry logic\n                for file in files:\n                    if file['mimeType'] != 'application/vnd.google-apps.folder':\n                        copy_metadata = {\n                            'name': file['name'],\n                            'parents': [task_folder_id]\n                        }\n                        \n                        self._thread_safe_api_call(\n                            lambda svc: svc.files().copy(\n                                fileId=file['id'],\n                                body=copy_metadata\n                            ).execute()\n                        )\n                        \n                        copied_count += 1\n                \n                duration = time.time() - start_time\n                \n                # Update progress\n                with self.progress_lock:\n                    self.completed_count += 1\n                    print(f\"‚úÖ ({self.completed_count}) {task['folder_name']}: {copied_count} files in {duration:.1f}s\")\n                \n                return {\n                    'task_id': task['task_id'],\n                    'folder_name': task['folder_name'],\n                    'success': True,\n                    'file_count': copied_count,\n                    'duration': duration,\n                    'task_folder_id': task_folder_id\n                }\n                \n            except Exception as e:\n                if attempt < max_retries - 1:\n                    # Reset services on SSL errors\n                    error_str = str(e).lower()\n                    if any(keyword in error_str for keyword in [\n                        'ssl', 'wrong_version_number', 'decryption_failed', 'incompleteread'\n                    ]):\n                        print(f\"üîÑ SSL error detected, resetting services for thread {threading.current_thread().ident}\")\n                        reset_thread_services()\n                    \n                    delay = base_delay * (2 ** attempt) + random.uniform(0, 1)\n                    print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed for {task['folder_name']}: {str(e)}\")\n                    print(f\"   Retrying in {delay:.1f}s...\")\n                    time.sleep(delay)\n                    continue\n                else:\n                    # Final attempt failed\n                    duration = time.time() - start_time\n                    \n                    with self.progress_lock:\n                        self.completed_count += 1\n                        print(f\"‚ùå ({self.completed_count}) {task['folder_name']}: Failed after {max_retries} attempts - {str(e)}\")\n                    \n                    return {\n                        'task_id': task['task_id'],\n                        'folder_name': task['folder_name'],\n                        'success': False,\n                        'error': str(e),\n                        'duration': duration\n                    }\n    \n    def _thread_safe_api_call(self, api_call_func, max_retries=3):\n        \"\"\"Thread-safe API call with service recreation on SSL errors\"\"\"\n        for attempt in range(max_retries):\n            try:\n                drive_service = get_thread_safe_drive_service()\n                return api_call_func(drive_service)\n            except Exception as e:\n                if attempt < max_retries - 1:\n                    # Check if it's a retryable error\n                    error_str = str(e).lower()\n                    if any(keyword in error_str for keyword in [\n                        'ssl', 'connection', 'timeout', 'incomplete', 'network',\n                        'internal error', 'service unavailable', 'rate limit',\n                        'wrong_version_number', 'decryption_failed'\n                    ]):\n                        print(f\"üîÑ Recreating service due to: {e}\")\n                        reset_thread_services()\n                        delay = (2 ** attempt) + random.uniform(0, 1)\n                        time.sleep(delay)\n                        continue\n                raise\n    \n    def stage_tasks_concurrent(self, tasks, batch_folder_id, max_workers=2):\n        \"\"\"Stage tasks with thread-safe concurrency (SAFER)\"\"\"\n        self.completed_count = 0\n        results = []\n        \n        # Very conservative settings to prevent SSL issues\n        chunk_size = 5  # Smaller chunks\n        actual_workers = min(max_workers, 2)  # Maximum 2 workers\n        \n        print(f\"üöÄ Starting CONCURRENT staging of {len(tasks)} tasks\")\n        print(f\"üõ°Ô∏è Thread-safe mode: {actual_workers} workers, chunks of {chunk_size}\")\n        print(f\"üìä Progress:\")\n        \n        total_start_time = time.time()\n        \n        # Process in very small chunks\n        for chunk_start in range(0, len(tasks), chunk_size):\n            chunk_end = min(chunk_start + chunk_size, len(tasks))\n            chunk_tasks = tasks[chunk_start:chunk_end]\n            \n            print(f\"\\nüîÑ Processing chunk {chunk_start//chunk_size + 1}: tasks {chunk_start+1}-{chunk_end}\")\n            \n            chunk_start_time = time.time()\n            \n            with ThreadPoolExecutor(max_workers=actual_workers) as executor:\n                # Submit chunk tasks\n                future_to_task = {\n                    executor.submit(self.copy_task_folder, task, batch_folder_id): task \n                    for task in chunk_tasks\n                }\n                \n                # Collect results as they complete\n                for future in as_completed(future_to_task):\n                    result = future.result()\n                    results.append(result)\n            \n            chunk_duration = time.time() - chunk_start_time\n            successful_in_chunk = sum(1 for r in results[-len(chunk_tasks):] if r['success'])\n            \n            print(f\"‚úÖ Chunk complete: {successful_in_chunk}/{len(chunk_tasks)} successful in {chunk_duration:.1f}s\")\n            \n            # Longer pause between chunks for maximum stability\n            if chunk_end < len(tasks):\n                print(\"‚è≥ Extended pause for API stability...\")\n                time.sleep(10)  # 10 second pause\n        \n        total_duration = time.time() - total_start_time\n        successful_count = sum(1 for r in results if r['success'])\n        failed_count = len(results) - successful_count\n        \n        print(f\"\\nüéØ Concurrent Staging Complete:\")\n        print(f\"   ‚úÖ Successful: {successful_count}\")\n        print(f\"   ‚ùå Failed: {failed_count}\")\n        print(f\"   ‚è±Ô∏è Total time: {total_duration:.1f}s\")\n        if total_duration > 0:\n            print(f\"   üìà Average speed: {len(tasks)/total_duration:.1f} tasks/second\")\n        \n        return results\n    \n    def stage_tasks_sequential(self, tasks, batch_folder_id):\n        \"\"\"Stage tasks one by one (SAFEST - no SSL conflicts)\"\"\"\n        self.completed_count = 0\n        results = []\n        \n        print(f\"üêå Starting SEQUENTIAL staging of {len(tasks)} tasks\")\n        print(f\"üõ°Ô∏è Maximum stability mode - no concurrency\")\n        print(f\"üìä Progress:\")\n        \n        total_start_time = time.time()\n        \n        for i, task in enumerate(tasks, 1):\n            print(f\"\\nüìÅ Processing task {i}/{len(tasks)}: {task['folder_name']}\")\n            \n            result = self.copy_task_folder(task, batch_folder_id)\n            results.append(result)\n            \n            # Small delay between tasks to be gentle on the API\n            if i < len(tasks):\n                time.sleep(2)\n        \n        total_duration = time.time() - total_start_time\n        successful_count = sum(1 for r in results if r['success'])\n        failed_count = len(results) - successful_count\n        \n        print(f\"\\nüéØ Sequential Staging Complete:\")\n        print(f\"   ‚úÖ Successful: {successful_count}\")\n        print(f\"   ‚ùå Failed: {failed_count}\")\n        print(f\"   ‚è±Ô∏è Total time: {total_duration:.1f}s\")\n        if total_duration > 0:\n            print(f\"   üìà Average speed: {len(tasks)/total_duration:.1f} tasks/second\")\n        \n        return results\n\nprint(\"üîß Thread-safe DriveOperations ready with concurrent AND sequential modes.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Status Updater with Thread-Safe Services and Fixed Column Detection\n\nclass StatusUpdater:\n    def __init__(self, sheets_service, sheet_id):\n        # Don't store the service - use thread-safe getter instead\n        self.sheet_id = sheet_id\n        \n    def update_export_statuses(self, staging_results, export_batch_id):\n        \"\"\"Update export status and batch ID for staged tasks\"\"\"\n        try:\n            # Use thread-safe sheets service\n            sheets_service = get_thread_safe_sheets_service()\n            \n            # First, get current sheet data to find row positions - SAME RANGE as TaskReader\n            range_name = 'Tasks!A:AZ'  # Match TaskReader exactly\n            result = sheets_service.spreadsheets().values().get(\n                spreadsheetId=self.sheet_id,\n                range=range_name\n            ).execute()\n            \n            values = result.get('values', [])\n            if not values:\n                print(\"‚ùå No data found in Tasks sheet\")\n                return\n                \n            headers = values[0]\n            \n            print(f\"üîç DEBUG: Found {len(headers)} columns in sheet\")\n            print(f\"üîç DEBUG: First 10 headers: {headers[:10] if len(headers) >= 10 else headers}\")\n            print(f\"üîç DEBUG: Last 10 headers: {headers[-10:] if len(headers) >= 10 else headers}\")\n            \n            # Find column indices using same logic as TaskReader\n            task_id_col = self._find_column_index(headers, 'Task ID')\n            export_status_col = self._find_column_index(headers, 'Export Status')\n            export_batch_col = self._find_column_index(headers, 'Export Batch')\n            export_time_col = self._find_column_index(headers, 'Export Time')\n            staged_count_col = self._find_column_index(headers, 'Staged Count')\n            \n            print(f\"üîç Column positions:\")\n            print(f\"   Task ID: {task_id_col} ({'Found' if task_id_col >= 0 else 'NOT FOUND'})\") \n            print(f\"   Export Status: {export_status_col} ({'Found' if export_status_col >= 0 else 'NOT FOUND'})\")\n            print(f\"   Export Batch: {export_batch_col} ({'Found' if export_batch_col >= 0 else 'NOT FOUND'})\") \n            print(f\"   Export Time: {export_time_col} ({'Found' if export_time_col >= 0 else 'NOT FOUND'})\") \n            print(f\"   Staged Count: {staged_count_col} ({'Found' if staged_count_col >= 0 else 'NOT FOUND'})\") \n            \n            # Try to find Export Status with variations if not found\n            if export_status_col == -1:\n                print(\"üîç Trying to find Export Status with variations...\")\n                possible_names = ['Export Status', 'export status', 'Export_Status', 'export_status', 'ExportStatus']\n                for name in possible_names:\n                    col_idx = self._find_column_index(headers, name)\n                    if col_idx >= 0:\n                        print(f\"‚úÖ Found Export Status as '{name}' at column {col_idx}\")\n                        export_status_col = col_idx\n                        break\n                \n                # If still not found, show all headers for debugging\n                if export_status_col == -1:\n                    print(\"‚ùå Export Status column still not found. All headers:\")\n                    for i, header in enumerate(headers):\n                        print(f\"   {i}: '{header}'\")\n                    print(\"‚ö†Ô∏è Warning: Export Status column not found - no status updates will be made\")\n                    return\n            \n            # Create batch updates\n            batch_updates = []\n            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            \n            successful_updates = 0\n            \n            for result in staging_results:\n                task_id = result['task_id']\n                \n                # Find the row for this task\n                row_index = None\n                for i, row in enumerate(values[1:], start=2):  # Start at row 2 (1-indexed)\n                    if len(row) > task_id_col and row[task_id_col] == task_id:\n                        row_index = i\n                        break\n                \n                if row_index:\n                    # Update export status\n                    new_status = 'staged' if result['success'] else 'staging_failed'\n                    \n                    # Export Status update\n                    status_range = self._get_column_letter(export_status_col) + str(row_index)\n                    batch_updates.append({\n                        'range': f'Tasks!{status_range}',\n                        'values': [[new_status]]\n                    })\n                    \n                    # Export Batch ID update  \n                    if export_batch_col >= 0:\n                        batch_range = self._get_column_letter(export_batch_col) + str(row_index)\n                        batch_updates.append({\n                            'range': f'Tasks!{batch_range}',\n                            'values': [[export_batch_id]]\n                        })\n                    \n                    # Export Time update\n                    if export_time_col >= 0:\n                        time_range = self._get_column_letter(export_time_col) + str(row_index)\n                        batch_updates.append({\n                            'range': f'Tasks!{time_range}',\n                            'values': [[current_time]]\n                        })\n                    \n                    # Staged Count update (for successful tasks)\n                    if staged_count_col >= 0 and result['success']:\n                        count_range = self._get_column_letter(staged_count_col) + str(row_index)\n                        file_count = result.get('file_count', 0)\n                        batch_updates.append({\n                            'range': f'Tasks!{count_range}',\n                            'values': [[str(file_count)]]\n                        })\n                    \n                    successful_updates += 1\n                else:\n                    print(f\"‚ö†Ô∏è Could not find row for task ID: {task_id}\")\n            \n            # Execute batch update\n            if batch_updates:\n                body = {\n                    'valueInputOption': 'RAW',\n                    'data': batch_updates\n                }\n                \n                sheets_service.spreadsheets().values().batchUpdate(\n                    spreadsheetId=self.sheet_id,\n                    body=body\n                ).execute()\n                \n                print(f\"üìù Updated {len(batch_updates)} sheet cells for {successful_updates} tasks\")\n                print(f\"‚úÖ Export status tracking completed successfully\")\n            else:\n                print(\"‚ö†Ô∏è No updates to make - check task IDs match between staging results and sheet\")\n            \n        except Exception as e:\n            print(f\"‚ùå Failed to update sheet statuses: {str(e)}\")\n            print(f\"üîç Error details: {e}\")\n            # Don't raise - staging succeeded even if status update failed\n    \n    def _find_column_index(self, headers, column_name):\n        \"\"\"Find column index by name (case-insensitive)\"\"\"\n        for i, header in enumerate(headers):\n            if header and header.strip().lower() == column_name.lower():\n                return i\n        return -1\n    \n    def _get_column_letter(self, col_index):\n        \"\"\"Convert column index to Excel-style column letter (0->A, 25->Z, 26->AA, etc.)\"\"\"\n        if col_index < 26:\n            return chr(65 + col_index)  # A-Z\n        else:\n            # For columns beyond Z (AA, AB, AC, etc.)\n            first_letter = chr(65 + (col_index // 26) - 1)\n            second_letter = chr(65 + (col_index % 26))\n            return first_letter + second_letter\n\nprint(\"üìä StatusUpdater ready with improved column detection and debugging.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: CLI Interface - Main Staging Workflow with Concurrency Toggle\n\ndef run_staging_workflow(use_concurrent=True):\n    \"\"\"Main CLI-driven staging workflow with concurrency options\"\"\"\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"üéØ 3D DATA MANAGEMENT - STAGING OPERATIONS\")\n    print(\"=\"*60)\n    \n    try:\n        # Step 1: Use hardcoded Sheet ID\n        sheet_id = SHEET_ID\n        if sheet_id == \"your_sheet_id_here\":\n            print(\"‚ùå Please update SHEET_ID in Cell 1 with your actual Google Sheet ID\")\n            return\n        \n        print(f\"üìã Using Sheet ID: {sheet_id}\")\n        \n        # Step 2: Load Configuration - use thread-safe service\n        print(\"\\n‚öôÔ∏è Loading configuration...\")\n        config_manager = ConfigManager(None, sheet_id)  # Pass None, it will use thread-safe getter\n        config = config_manager.load_config()\n        \n        # Step 3: Get CLI Parameters\n        print(\"\\nüîç Staging filters:\")\n        batch_id_filter = input(\"   Batch ID (optional): \").strip() or None\n        agent_filter = input(\"   Agent email contains (optional): \").strip() or None\n        \n        # Step 4: Load and Filter Tasks - use thread-safe service\n        print(\"\\nüìñ Reading tasks from sheet...\")\n        task_reader = TaskReader(None, sheet_id)  # Pass None, it will use thread-safe getter\n        tasks = task_reader.get_stageable_tasks(batch_id_filter, agent_filter)\n        \n        if not tasks:\n            print(\"\\n‚ùå No tasks found matching criteria\")\n            print(\"   Make sure tasks have status='complete' and review_status='passed'\")\n            return\n        \n        # Step 5: Preview Tasks\n        print(f\"\\nüìã Found {len(tasks)} tasks ready for staging:\")\n        for i, task in enumerate(tasks[:5], 1):\n            print(f\"   {i}. {task['folder_name']} (Group: {task.get('group', 'N/A')})\")\n        if len(tasks) > 5:\n            print(f\"   ... and {len(tasks) - 5} more\")\n        \n        # Step 6: Choose Processing Mode\n        if not use_concurrent:\n            # Skip user prompt if mode is pre-determined\n            concurrent_mode = False\n        else:\n            print(\"\\nüöÄ Choose processing mode:\")\n            print(\"   [C] Concurrent - Faster but may have SSL connection issues\")\n            print(\"   [S] Sequential - Slower but maximum stability (RECOMMENDED)\")\n            mode_choice = input(\"   Choose mode [C/S]: \").strip().upper()\n            \n            concurrent_mode = mode_choice == 'C'\n        \n        mode_name = \"CONCURRENT\" if concurrent_mode else \"SEQUENTIAL\"\n        print(f\"   Selected: {mode_name} mode\")\n        \n        # Step 7: Confirm Staging\n        confirm = input(f\"\\nüöÄ Stage {len(tasks)} tasks in {mode_name} mode? [y/N]: \").strip().lower()\n        if confirm != 'y':\n            print(\"‚ùå Staging cancelled\")\n            return\n        \n        # Step 8: Generate Export Batch ID\n        export_batch_id = input(\"\\nüè∑Ô∏è Enter export batch ID (or press Enter for auto-generated): \").strip()\n        if not export_batch_id:\n            export_batch_id = f\"staging_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        \n        print(f\"   Using batch ID: {export_batch_id}\")\n        \n        # Step 9: Create Batch Folder\n        print(\"\\nüìÅ Creating batch folder...\")\n        drive_ops = DriveOperations()\n        batch_folder = drive_ops.create_batch_folder(\n            config['STAGING_FOLDER_ID'], \n            export_batch_id\n        )\n        \n        # Step 10: Stage Tasks with Selected Mode\n        print(f\"\\nüî• Starting {mode_name} staging...\")\n        \n        if concurrent_mode:\n            # Concurrent mode with very conservative settings\n            max_workers = min(2, len(tasks))  # Maximum 2 workers\n            staging_results = drive_ops.stage_tasks_concurrent(\n                tasks, \n                batch_folder['id'], \n                max_workers=max_workers\n            )\n        else:\n            # Sequential mode - maximum stability\n            staging_results = drive_ops.stage_tasks_sequential(\n                tasks, \n                batch_folder['id']\n            )\n        \n        # Step 11: Update Sheet Statuses - use thread-safe service\n        print(\"\\nüìä Updating sheet statuses...\")\n        status_updater = StatusUpdater(None, sheet_id)  # Pass None, it will use thread-safe getter\n        status_updater.update_export_statuses(staging_results, export_batch_id)\n        \n        # Step 12: Final Summary\n        successful_tasks = [r for r in staging_results if r['success']]\n        failed_tasks = [r for r in staging_results if not r['success']]\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"üéâ STAGING COMPLETE\")\n        print(\"=\"*60)\n        print(f\"üìÅ Batch Folder: {batch_folder['name']}\")\n        print(f\"üîó Folder URL: {batch_folder['url']}\")\n        print(f\"‚öôÔ∏è Processing Mode: {mode_name}\")\n        print(f\"‚úÖ Successful: {len(successful_tasks)}\")\n        print(f\"‚ùå Failed: {len(failed_tasks)}\")\n        \n        if failed_tasks:\n            print(\"\\n‚ùå Failed Tasks:\")\n            for task in failed_tasks:\n                print(f\"   ‚Ä¢ {task['folder_name']}: {task['error']}\")\n        \n        if len(failed_tasks) > 0 and concurrent_mode:\n            print(f\"\\nüí° TIP: If you see SSL errors, try running again with sequential mode:\")\n            print(f\"   run_staging_workflow(use_concurrent=False)\")\n        \n        print(\"\\nüèÅ Staging operation completed successfully!\")\n        \n    except Exception as e:\n        print(f\"\\nüí• Staging failed: {str(e)}\")\n        raise\n\n# Ready to run\nprint(\"\\nüéØ Enhanced CLI Staging Workflow ready!\")\nprint(\"üìû Run: run_staging_workflow()  # Interactive mode selection\")\nprint(\"üìû Or: run_staging_workflow(use_concurrent=False)  # Force sequential mode\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Execute Staging with Configuration\n\n# CONFIGURATION - Edit these settings as needed\nUSE_CONCURRENT = False  # Set to True for concurrent mode, False for sequential mode\n\n# Mode explanations:\n# - Sequential (False): Processes tasks one by one. Slower but extremely stable.\n#   Use this mode if you experience SSL connection errors or need maximum reliability.\n#\n# - Concurrent (True): Processes multiple tasks simultaneously with 2 workers max.\n#   Faster but may encounter SSL connection issues in unstable network environments.\n#   Uses thread-safe service instances to minimize conflicts.\n\nprint(\"üéØ STAGING CONFIGURATION:\")\nprint(f\"   Mode: {'CONCURRENT' if USE_CONCURRENT else 'SEQUENTIAL'}\")\nprint(f\"   SSL Protection: Thread-safe services enabled\")\n\nif USE_CONCURRENT:\n    print(\"   ‚ö†Ô∏è  Using concurrent mode - monitor for SSL errors\")\n    print(\"   üí° If you see SSL errors, change USE_CONCURRENT = False above\")\nelse:\n    print(\"   üõ°Ô∏è  Using sequential mode - maximum stability\")\n\nprint(\"\\n\" + \"=\"*50)\n\n# Run the staging workflow with the configured mode\nrun_staging_workflow(use_concurrent=USE_CONCURRENT)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}