{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 3D Data Management - Health Check System\n",
    "\n",
    "Analyzes production folder health at hyperspeed with surgical precision.\n",
    "Detects duplicate versions, duplicate content, missing files, and naming violations.\n",
    "\n",
    "## Health Parameters\n",
    "- **No Duplicate Versions**: One file per type per version (image_v1, mesh_v1, video_v1)\n",
    "- **No Duplicate Content**: MD5 hash verification prevents double-upload artifacts\n",
    "- **Complete File Sets**: Each version has image + mesh + video(s)\n",
    "- **Sequential Videos**: No gaps in video versions (v1.1, v1.2, v1.3...)\n",
    "- **Proper Naming**: All files match folder naming convention\n",
    "\n",
    "**Architecture**: Stability at Hyperspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "source": "# Cell 1: Authentication and Setup\nimport os\nimport re\nimport time\nimport random\nimport threading\nfrom datetime import datetime\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom threading import Lock\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Google API imports\nfrom google.colab import auth\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nimport pandas as pd\nimport google.auth\n\n# Configuration\nPRODUCTION_FOLDER_ID = \"1abc123def456\"  # Replace with actual production folder ID\nSHEET_ID = \"1HmDdq5g0Zk7d7Uodbh7fIUFq5XiONDZyvrBBZQgNK0w\"\nDELIVERABLE_TYPES = ['recording', 'screenshot', 'mesh']\n\n# FIXED: Corrected patterns to properly ignore original client files\nIGNORED_PATTERNS = [\n    r'^image\\.jpg$', \n    r'^mask\\.jpg$', \n    r'^img_mask\\.jpg$',  # Fixed: was img-mask, should be img_mask\n    r'_v0\\.'\n]\n\n# Global state\n_credentials = None\nthread_local = threading.local()\n\ndef get_credentials():\n    global _credentials\n    if _credentials is None:\n        _credentials, _ = google.auth.default()\n    return _credentials\n\ndef get_drive_service():\n    if not hasattr(thread_local, 'drive'):\n        thread_local.drive = build('drive', 'v3', credentials=get_credentials())\n    return thread_local.drive\n\ndef get_sheets_service():\n    if not hasattr(thread_local, 'sheets'):\n        thread_local.sheets = build('sheets', 'v4', credentials=get_credentials())\n    return thread_local.sheets\n\nprint(\"🔐 Authenticating...\")\nauth.authenticate_user()\n_credentials = get_credentials()\nprint(\"✅ Authentication established\")\nprint(f\"📋 Sheet ID: {SHEET_ID}\")\nprint(f\"📁 Production Folder: {PRODUCTION_FOLDER_ID}\")\nprint(\"🚫 Ignoring original client files: image.jpg, mask.jpg, img_mask.jpg\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "source": "# Cell 2: File Parser - Atomic file analysis with PNG support\n\nclass FileParser:\n    @staticmethod\n    def is_deliverable(filename):\n        \"\"\"Swift determination of file relevance\"\"\"\n        return not any(re.search(pattern, filename, re.IGNORECASE) \n                      for pattern in IGNORED_PATTERNS)\n    \n    @staticmethod\n    def parse_deliverable(filename):\n        \"\"\"Extract essence from filename chaos - FIXED: Now supports PNG screenshots\"\"\"\n        if not FileParser.is_deliverable(filename):\n            return None\n            \n        # FIXED: Updated pattern to support PNG and other image extensions for screenshots\n        # Pattern breakdown: folder_name + type + version + extension\n        pattern = r'^(.+)_(recording|screenshot|mesh)_(v\\d+(?:\\.\\d+)?)\\.(mp4|jpg|jpeg|png|obj)$'\n        match = re.match(pattern, filename, re.IGNORECASE)\n        \n        if not match:\n            return {'valid': False, 'filename': filename}\n            \n        folder_name, file_type, version, extension = match.groups()\n        major, minor = FileParser.extract_version_parts(version)\n        \n        # Validate extension matches file type\n        valid_extensions = {\n            'recording': ['mp4'],\n            'screenshot': ['jpg', 'jpeg', 'png'],  # FIXED: Added PNG support\n            'mesh': ['obj']\n        }\n        \n        if extension.lower() not in valid_extensions.get(file_type, []):\n            return {'valid': False, 'filename': filename}\n        \n        return {\n            'valid': True,\n            'filename': filename,\n            'folder_name': folder_name,\n            'type': file_type,\n            'version': version,\n            'major': major,\n            'minor': minor,\n            'extension': extension.lower()\n        }\n    \n    @staticmethod\n    def extract_version_parts(version_str):\n        \"\"\"Dissect version into numerical components\"\"\"\n        version_clean = version_str[1:]  # Remove 'v'\n        \n        if '.' in version_clean:\n            major, minor = map(int, version_clean.split('.'))\n            return major, minor\n        \n        return int(version_clean), 0\n\n# Test parser elegance with corrected expectations\ntest_files = [\n    \"mc_0_1300_screenshot_v1.jpg\",     # Should work\n    \"sa3_1576545853_handsaw_2_screenshot_v1.png\",  # FIXED: Should now work with PNG\n    \"mc_0_1300_recording_v1.2.mp4\",   # Should work\n    \"mc_0_1300_mesh_v1.obj\",          # Should work\n    \"image.jpg\",                       # Should be ignored\n    \"img_mask.jpg\",                    # FIXED: Should be ignored\n    \"mc_0_1300_mesh_v0.obj\"           # Should be ignored (v0)\n]\n\nprint(\"🔍 File Parser ready with PNG support. Testing precision:\")\nfor test_file in test_files:\n    result = FileParser.parse_deliverable(test_file)\n    if result:\n        status = \"✅\" if result.get('valid') else \"⚠️\"\n        print(f\"   {status} {test_file} → {result}\")\n    else:\n        print(f\"   🚫 {test_file} → ignored (correct)\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "source": "# Cell 3: Health Analyzers - Surgical issue detection with fixed recording logic\n\nclass HealthAnalyzer:\n    @staticmethod\n    def check_duplicate_versions(files):\n        \"\"\"Detect version collisions with crystalline clarity\"\"\"\n        version_map = defaultdict(list)\n        \n        for file in files:\n            parsed = file.get('parsed')\n            if parsed and parsed.get('valid'):\n                key = f\"{parsed['type']}_{parsed['version']}\"\n                version_map[key].append(file['name'])\n        \n        return [\n            {\n                'type': 'duplicate_version',\n                'version_key': key,\n                'files': filenames,\n                'count': len(filenames)\n            }\n            for key, filenames in version_map.items() \n            if len(filenames) > 1\n        ]\n    \n    @staticmethod\n    def check_duplicate_content(files):\n        \"\"\"MD5 truth reveals hidden duplicates\"\"\"\n        md5_groups = defaultdict(list)\n        \n        for file in files:\n            if file.get('md5Checksum') and file.get('size'):\n                key = f\"{file['md5Checksum']}_{file['size']}\"\n                md5_groups[key].append(file)\n        \n        duplicates = []\n        \n        for files_group in md5_groups.values():\n            if len(files_group) > 1:\n                # Check if created within suspicious timeframe\n                times = [datetime.fromisoformat(f['createdTime'].replace('Z', '+00:00')) \n                        for f in files_group]\n                times.sort()\n                \n                for i in range(1, len(times)):\n                    time_diff = (times[i] - times[i-1]).total_seconds()\n                    if time_diff < 300:  # 5 minutes\n                        duplicates.append({\n                            'type': 'duplicate_content',\n                            'files': [f['name'] for f in files_group],\n                            'md5': files_group[0]['md5Checksum'],\n                            'size': files_group[0]['size'],\n                            'time_gap_seconds': int(time_diff)\n                        })\n                        break\n        \n        return duplicates\n    \n    @staticmethod\n    def check_missing_files(files, folder_name):\n        \"\"\"Illuminate the void where files should exist\"\"\"\n        versions = defaultdict(lambda: {'screenshot': None, 'mesh': None, 'recordings': []})\n        issues = []\n        \n        # Categorize files by version\n        for file in files:\n            parsed = file.get('parsed')\n            if parsed and parsed.get('valid'):\n                version = f\"v{parsed['major']}\"\n                if parsed['type'] == 'recording':\n                    versions[version]['recordings'].append(parsed)\n                else:\n                    versions[version][parsed['type']] = parsed\n        \n        # Check completeness\n        for version, files_by_type in versions.items():\n            # Minimum file requirements\n            if not files_by_type['screenshot']:\n                issues.append({'type': 'missing_file', 'version': version, 'file_type': 'screenshot'})\n            if not files_by_type['mesh']:\n                issues.append({'type': 'missing_file', 'version': version, 'file_type': 'mesh'})\n            if not files_by_type['recordings']:\n                issues.append({'type': 'missing_file', 'version': version, 'file_type': 'recording'})\n            \n            # FIXED: Check recording sequence gaps ONLY when there are multiple recordings\n            recordings = files_by_type['recordings']\n            if len(recordings) > 1:  # Only check gaps if there are 2+ recordings\n                minor_versions = sorted([v['minor'] for v in recordings])\n                \n                # Only flag gaps if we have recordings with minor versions (v1.1, v1.2, etc.)\n                has_minor_versions = any(minor > 0 for minor in minor_versions)\n                \n                if has_minor_versions:\n                    # Find the starting point for sequence checking\n                    min_minor = min(minor for minor in minor_versions if minor > 0)\n                    \n                    for i in range(len(minor_versions) - 1):\n                        current_minor = minor_versions[i]\n                        next_minor = minor_versions[i + 1]\n                        \n                        # Only check gaps for minor versions (not v1 to v1.1 jump)\n                        if current_minor > 0 and next_minor > current_minor + 1:\n                            expected = current_minor + 1\n                            issues.append({\n                                'type': 'recording_gap',\n                                'version': version,\n                                'missing': f\"{version}.{expected}\"\n                            })\n                            break  # Only report first gap\n            \n            # Note: Single recording (v1 only) is perfectly valid - no gap check needed\n        \n        return issues\n    \n    @staticmethod\n    def check_naming_convention(files, folder_name):\n        \"\"\"Enforce naming discipline with gentle firmness\"\"\"\n        violations = []\n        \n        for file in files:\n            parsed = file.get('parsed')\n            # Only check files that passed the is_deliverable filter\n            if parsed and not parsed.get('valid'):\n                violations.append({\n                    'type': 'naming_violation',\n                    'filename': parsed['filename'],\n                    'issue': 'invalid_format'\n                })\n            elif parsed and parsed.get('valid'):\n                if parsed['folder_name'] != folder_name:\n                    violations.append({\n                        'type': 'naming_violation',\n                        'filename': parsed['filename'],\n                        'issue': 'folder_mismatch',\n                        'expected': f\"{folder_name}_...\",\n                        'actual': f\"{parsed['folder_name']}_...\"\n                    })\n        \n        return violations\n\nprint(\"🔬 Health Analyzers calibrated with FIXED recording gap logic\")\nprint(\"✅ Single recordings (v1 only) are now considered healthy\")\nprint(\"⚠️ Gaps only flagged when multiple recordings exist with missing sequences\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "source": "# Cell 4: Folder Scanner - Concurrent exploration with enhanced debugging\n\nclass FolderScanner:\n    def __init__(self):\n        self.progress_lock = Lock()\n        self.completed_count = 0\n    \n    def scan_folder(self, folder_id, folder_name):\n        \"\"\"Deep scan single folder with MD5 precision\"\"\"\n        try:\n            drive = get_drive_service()\n            \n            # List all files\n            query = f\"'{folder_id}' in parents and trashed=false\"\n            fields = 'files(id,name,size,md5Checksum,createdTime,mimeType)'\n            \n            result = drive.files().list(q=query, fields=fields).execute()\n            raw_files = result.get('files', [])\n            \n            # ENHANCED: Better debugging of file processing\n            total_raw_files = len(raw_files)\n            ignored_files = []\n            deliverable_files = []\n            \n            # Filter and parse deliverable files\n            files = []\n            for raw_file in raw_files:\n                if raw_file['mimeType'] != 'application/vnd.google-apps.folder':\n                    filename = raw_file['name']\n                    \n                    # Check if file should be ignored\n                    if not FileParser.is_deliverable(filename):\n                        ignored_files.append(filename)\n                        continue\n                    \n                    # Parse the deliverable file\n                    parsed = FileParser.parse_deliverable(filename)\n                    files.append({\n                        **raw_file,\n                        'parsed': parsed\n                    })\n                    \n                    if parsed and parsed.get('valid'):\n                        deliverable_files.append(filename)\n            \n            # Debug logging for first few folders\n            if self.completed_count < 3:  # Debug first 3 folders\n                print(f\"🔍 DEBUG {folder_name}:\")\n                print(f\"   Total files: {total_raw_files}\")\n                print(f\"   Ignored files: {len(ignored_files)} {ignored_files[:3]}\")\n                print(f\"   Valid deliverables: {len(deliverable_files)} {deliverable_files[:3]}\")\n            \n            # Run health checks\n            issues = []\n            issues.extend(HealthAnalyzer.check_duplicate_versions(files))\n            issues.extend(HealthAnalyzer.check_duplicate_content(files))\n            issues.extend(HealthAnalyzer.check_missing_files(files, folder_name))\n            issues.extend(HealthAnalyzer.check_naming_convention(files, folder_name))\n            \n            # Calculate health score\n            deliverable_count = len([f for f in files if f.get('parsed') and f['parsed'].get('valid')])\n            health_score = max(0, 1 - (len(issues) / max(deliverable_count, 1))) if deliverable_count > 0 else 1.0\n            \n            with self.progress_lock:\n                self.completed_count += 1\n                status = \"✅\" if not issues else \"⚠️\"\n                print(f\"{status} ({self.completed_count}) {folder_name}: {len(issues)} issues, score {health_score:.2f}, {deliverable_count} deliverables\")\n            \n            return {\n                'folder_name': folder_name,\n                'folder_id': folder_id,\n                'health_score': health_score,\n                'total_files': deliverable_count,\n                'raw_file_count': total_raw_files,\n                'ignored_file_count': len(ignored_files),\n                'issues': issues,\n                'scan_time': datetime.now().isoformat()\n            }\n            \n        except Exception as e:\n            with self.progress_lock:\n                self.completed_count += 1\n                print(f\"❌ ({self.completed_count}) {folder_name}: Scan failed - {e}\")\n            \n            return {\n                'folder_name': folder_name,\n                'folder_id': folder_id,\n                'health_score': 0,\n                'error': str(e),\n                'scan_time': datetime.now().isoformat()\n            }\n    \n    def batch_scan_folders(self, folder_data, max_workers=3):\n        \"\"\"Orchestrate concurrent scans with surgical precision\"\"\"\n        self.completed_count = 0\n        results = []\n        \n        chunk_size = 10\n        actual_workers = min(max_workers, 3)\n        \n        print(f\"🚀 Scanning {len(folder_data)} folders with {actual_workers} workers\")\n        print(f\"📊 Processing in chunks of {chunk_size}\")\n        \n        start_time = time.time()\n        \n        # Process in chunks for memory management\n        for chunk_start in range(0, len(folder_data), chunk_size):\n            chunk_end = min(chunk_start + chunk_size, len(folder_data))\n            chunk_folders = folder_data[chunk_start:chunk_end]\n            \n            print(f\"\\n🔄 Chunk {chunk_start//chunk_size + 1}: folders {chunk_start+1}-{chunk_end}\")\n            \n            with ThreadPoolExecutor(max_workers=actual_workers) as executor:\n                futures = {\n                    executor.submit(self.scan_folder, folder['id'], folder['name']): folder\n                    for folder in chunk_folders\n                }\n                \n                for future in as_completed(futures):\n                    result = future.result()\n                    results.append(result)\n            \n            # Gentle pause between chunks\n            if chunk_end < len(folder_data):\n                time.sleep(2)\n        \n        duration = time.time() - start_time\n        successful = sum(1 for r in results if 'error' not in r)\n        total_issues = sum(len(r.get('issues', [])) for r in results if 'error' not in r)\n        \n        print(f\"\\n🎯 Scan Complete: {successful}/{len(results)} successful in {duration:.1f}s\")\n        print(f\"🔍 Total issues found across all folders: {total_issues}\")\n        return results\n\nprint(\"📡 Folder Scanner ready with enhanced debugging for issue detection\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "source": "# Cell 5: Report Generator - Minimal, actionable intelligence with fixed CSV export\n\nclass ReportGenerator:\n    @staticmethod\n    def generate_summary(health_results):\n        \"\"\"Distill chaos into crystalline insights\"\"\"\n        total_folders = len(health_results)\n        successful_scans = [r for r in health_results if 'error' not in r]\n        failed_scans = total_folders - len(successful_scans)\n        \n        if not successful_scans:\n            return {'error': 'No successful scans to analyze'}\n        \n        # Calculate metrics\n        avg_health = sum(r['health_score'] for r in successful_scans) / len(successful_scans)\n        perfect_folders = sum(1 for r in successful_scans if r['health_score'] == 1.0)\n        \n        # Issue categorization\n        issue_counts = defaultdict(int)\n        critical_folders = []\n        \n        for result in successful_scans:\n            issues = result.get('issues', [])\n            if result['health_score'] < 0.5:\n                critical_folders.append(result['folder_name'])\n            \n            for issue in issues:\n                issue_counts[issue['type']] += 1\n        \n        return {\n            'scan_timestamp': datetime.now().isoformat(),\n            'total_folders': total_folders,\n            'successful_scans': len(successful_scans),\n            'failed_scans': failed_scans,\n            'average_health_score': round(avg_health, 3),\n            'perfect_folders': perfect_folders,\n            'critical_folders_count': len(critical_folders),\n            'critical_folders': critical_folders[:10],  # Top 10 worst\n            'issue_breakdown': dict(issue_counts),\n            'recommendations': ReportGenerator._generate_recommendations(issue_counts)\n        }\n    \n    @staticmethod\n    def _generate_recommendations(issue_counts):\n        \"\"\"Prescribe targeted remedies\"\"\"\n        recommendations = []\n        \n        if issue_counts.get('duplicate_content', 0) > 0:\n            recommendations.append(\"Remove duplicate files to reclaim storage space\")\n        \n        if issue_counts.get('missing_file', 0) > 0:\n            recommendations.append(\"Complete missing file sets before staging\")\n        \n        if issue_counts.get('recording_gap', 0) > 0:\n            recommendations.append(\"Fill video sequence gaps or remove orphaned versions\")\n        \n        if issue_counts.get('naming_violation', 0) > 0:\n            recommendations.append(\"Standardize file naming to match folder conventions\")\n        \n        return recommendations or [\"All folders meet health standards\"]\n    \n    @staticmethod\n    def export_csv(health_results, summary, filename=None):\n        \"\"\"Export for spreadsheet warriors - FIXED CSV field handling\"\"\"\n        if not filename:\n            filename = f\"health_check_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n        \n        rows = []\n        \n        # Define all possible CSV fields upfront\n        base_fields = [\n            'folder_name', 'health_score', 'total_files', 'issue_count', \n            'scan_time', 'error', 'issue_type', 'issue_details'\n        ]\n        \n        for result in health_results:\n            base_row = {\n                'folder_name': result['folder_name'],\n                'health_score': result['health_score'],\n                'total_files': result.get('total_files', 0),\n                'issue_count': len(result.get('issues', [])),\n                'scan_time': result['scan_time'],\n                'error': result.get('error', ''),\n                'issue_type': '',  # Initialize empty\n                'issue_details': ''  # Initialize empty\n            }\n            \n            issues = result.get('issues', [])\n            if issues:\n                # Create one row per issue\n                for issue in issues:\n                    row = base_row.copy()\n                    row['issue_type'] = issue['type']\n                    row['issue_details'] = json.dumps(issue)\n                    rows.append(row)\n            else:\n                # Create one row for folders with no issues\n                rows.append(base_row)\n        \n        # Write CSV with proper field handling\n        if rows:\n            with open(filename, 'w', newline='', encoding='utf-8') as f:\n                writer = csv.DictWriter(f, fieldnames=base_fields)\n                writer.writeheader()\n                writer.writerows(rows)\n        \n        print(f\"📄 CSV exported: {filename}\")\n        return filename\n    \n    @staticmethod\n    def export_json(health_results, summary, filename=None):\n        \"\"\"Export for programmers and APIs\"\"\"\n        if not filename:\n            filename = f\"health_check_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n        \n        export_data = {\n            'summary': summary,\n            'results': health_results\n        }\n        \n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(export_data, f, indent=2, ensure_ascii=False)\n        \n        print(f\"📋 JSON exported: {filename}\")\n        return filename\n\nprint(\"📊 Report Generator ready with FIXED CSV export field handling\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "source": [
    "# Cell 6: CLI Orchestrator - User-guided health assessment\n",
    "\n",
    "def get_stageable_folders():\n",
    "    \"\"\"Retrieve folders ready for health assessment\"\"\"\n",
    "    try:\n",
    "        sheets = get_sheets_service()\n",
    "        \n",
    "        range_name = 'Tasks!A:AZ'\n",
    "        result = sheets.spreadsheets().values().get(\n",
    "            spreadsheetId=SHEET_ID,\n",
    "            range=range_name\n",
    "        ).execute()\n",
    "        \n",
    "        values = result.get('values', [])\n",
    "        if not values:\n",
    "            return []\n",
    "        \n",
    "        headers = values[0]\n",
    "        data_rows = values[1:]\n",
    "        \n",
    "        # Pad rows to match headers\n",
    "        max_cols = max(len(headers), max(len(row) for row in data_rows) if data_rows else 0)\n",
    "        if len(headers) < max_cols:\n",
    "            headers.extend([f'Column_{i}' for i in range(len(headers), max_cols)])\n",
    "        \n",
    "        padded_rows = [row + [''] * (len(headers) - len(row)) for row in data_rows]\n",
    "        df = pd.DataFrame(padded_rows, columns=headers)\n",
    "        \n",
    "        # Filter criteria\n",
    "        df['status_clean'] = df['Status'].fillna('').astype(str).str.strip().str.lower()\n",
    "        df['review_clean'] = df['Review Status'].fillna('').astype(str).str.strip().str.lower()\n",
    "        \n",
    "        eligible = df[\n",
    "            (df['status_clean'] == 'complete') & \n",
    "            (df['review_clean'] == 'passed')\n",
    "        ]\n",
    "        \n",
    "        folders = []\n",
    "        for _, row in eligible.iterrows():\n",
    "            folder_link = row.get('Production Folder', '')\n",
    "            folder_id = extract_folder_id(folder_link)\n",
    "            if folder_id:\n",
    "                folders.append({\n",
    "                    'name': row['Folder Name'],\n",
    "                    'id': folder_id,\n",
    "                    'batch_id': row.get('Batch ID', ''),\n",
    "                    'agent': row.get('Agent Email', '')\n",
    "                })\n",
    "        \n",
    "        return folders\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load folders: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_folder_id(folder_link):\n",
    "    \"\"\"Extract Drive folder ID from URL\"\"\"\n",
    "    if not folder_link:\n",
    "        return None\n",
    "    \n",
    "    match = re.search(r'folders/([a-zA-Z0-9-_]+)', folder_link)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    if re.match(r'^[a-zA-Z0-9-_]{20,}$', folder_link):\n",
    "        return folder_link\n",
    "    \n",
    "    return None\n",
    "\n",
    "def run_health_check():\n",
    "    \"\"\"Orchestrate the complete health assessment workflow\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🏥 3D DATA MANAGEMENT - HEALTH CHECK SYSTEM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load eligible folders\n",
    "        print(\"\\n📋 Loading folders ready for health check...\")\n",
    "        folders = get_stageable_folders()\n",
    "        \n",
    "        if not folders:\n",
    "            print(\"❌ No folders found matching criteria (status=complete, review=passed)\")\n",
    "            return\n",
    "        \n",
    "        print(f\"📁 Found {len(folders)} folders eligible for health check\")\n",
    "        \n",
    "        # Step 2: Apply filters\n",
    "        batch_filter = input(\"\\n🔍 Filter by batch ID (optional): \").strip() or None\n",
    "        agent_filter = input(\"🔍 Filter by agent email contains (optional): \").strip() or None\n",
    "        \n",
    "        if batch_filter:\n",
    "            folders = [f for f in folders if f['batch_id'] == batch_filter]\n",
    "            print(f\"📊 After batch filter: {len(folders)} folders\")\n",
    "        \n",
    "        if agent_filter:\n",
    "            folders = [f for f in folders if agent_filter.lower() in f['agent'].lower()]\n",
    "            print(f\"📊 After agent filter: {len(folders)} folders\")\n",
    "        \n",
    "        if not folders:\n",
    "            print(\"❌ No folders remain after filtering\")\n",
    "            return\n",
    "        \n",
    "        # Step 3: Preview and confirm\n",
    "        print(f\"\\n📋 Folders to scan ({len(folders)}):\")\n",
    "        for i, folder in enumerate(folders[:5], 1):\n",
    "            print(f\"   {i}. {folder['name']} (Batch: {folder.get('batch_id', 'N/A')})\")\n",
    "        if len(folders) > 5:\n",
    "            print(f\"   ... and {len(folders) - 5} more\")\n",
    "        \n",
    "        confirm = input(f\"\\n🚀 Start health check on {len(folders)} folders? [y/N]: \").lower()\n",
    "        if confirm != 'y':\n",
    "            print(\"❌ Health check cancelled\")\n",
    "            return\n",
    "        \n",
    "        # Step 4: Choose export format\n",
    "        print(\"\\n📄 Export format:\")\n",
    "        print(\"   [1] Console only (fastest)\")\n",
    "        print(\"   [2] CSV file (spreadsheet-friendly)\")\n",
    "        print(\"   [3] JSON file (programmatic)\")\n",
    "        print(\"   [4] Both CSV and JSON\")\n",
    "        \n",
    "        export_choice = input(\"Choose export format [1]: \").strip() or \"1\"\n",
    "        \n",
    "        # Step 5: Execute health scan\n",
    "        print(f\"\\n🔬 Starting health assessment...\")\n",
    "        scanner = FolderScanner()\n",
    "        results = scanner.batch_scan_folders(folders)\n",
    "        \n",
    "        # Step 6: Generate summary\n",
    "        summary = ReportGenerator.generate_summary(results)\n",
    "        \n",
    "        # Step 7: Display results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📋 HEALTH CHECK SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"🔍 Total folders scanned: {summary['total_folders']}\")\n",
    "        print(f\"✅ Successful scans: {summary['successful_scans']}\")\n",
    "        print(f\"❌ Failed scans: {summary['failed_scans']}\")\n",
    "        print(f\"📊 Average health score: {summary['average_health_score']}\")\n",
    "        print(f\"🏆 Perfect folders: {summary['perfect_folders']}\")\n",
    "        print(f\"⚠️ Critical folders: {summary['critical_folders_count']}\")\n",
    "        \n",
    "        if summary.get('issue_breakdown'):\n",
    "            print(\"\\n🔍 Issue Breakdown:\")\n",
    "            for issue_type, count in summary['issue_breakdown'].items():\n",
    "                print(f\"   • {issue_type}: {count}\")\n",
    "        \n",
    "        if summary.get('recommendations'):\n",
    "            print(\"\\n💡 Recommendations:\")\n",
    "            for rec in summary['recommendations']:\n",
    "                print(f\"   • {rec}\")\n",
    "        \n",
    "        # Step 8: Export results\n",
    "        if export_choice in [\"2\", \"4\"]:\n",
    "            ReportGenerator.export_csv(results, summary)\n",
    "        \n",
    "        if export_choice in [\"3\", \"4\"]:\n",
    "            ReportGenerator.export_json(results, summary)\n",
    "        \n",
    "        print(\"\\n🏁 Health check completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n💥 Health check failed: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\n🎯 Health Check CLI ready for deployment\")\n",
    "print(\"📞 Execute: run_health_check()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "source": [
    "# Cell 7: Execute Health Check\n",
    "\n",
    "# Launch the health assessment workflow\n",
    "run_health_check()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}